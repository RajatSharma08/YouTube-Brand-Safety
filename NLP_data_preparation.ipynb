{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1362\n",
      "[('Sex', 46), ('Bhabhi', 41), ('Hot', 38), ('Video', 38), ('video', 36), ('The', 28), ('full', 24), ('sex', 24), ('Hindi', 23), ('sexy', 23), ('HD', 22), ('movie', 22), ('devar', 19), ('film', 19), ('xxx', 19), ('Sexy', 19), ('Devar', 19), ('Romance', 19), ('hot', 18), ('blue', 17), ('fliz', 17), ('Nude', 17), ('killed', 17), ('Trafficking', 16), ('Trashed', 16), ('Film', 15), ('Human', 15), ('Violence', 15), ('Blood', 15), ('Kiss', 14), ('Porn', 14), ('Movie', 13), ('Brutality', 13), ('Official', 12), ('Love', 12), ('MMS', 12), ('Guns', 12), ('In', 11), ('Police', 11), ('To', 9), ('Romantic', 9), ('Full', 9), ('How', 9), ('Trailer', 9), ('bhabhi', 9), ('New', 9), ('mms', 9), ('viral', 9), ('Rape', 9), ('Blue', 8), ('And', 8), ('Indian', 8), ('Desi', 8), ('Best', 8), ('Of', 8), ('VIDEO', 8), ('Videos', 7), ('With', 7), ('Short', 7), ('leaked', 7), ('Leaked', 7), ('Fuck', 7), ('police', 7), ('Riots', 7), ('Delhi', 7), ('riots', 7), ('Crime', 7), ('Patrol', 7), ('Nerf', 7), ('Part', 6), ('Kissing', 6), ('kiss', 6), ('My', 6), ('ki', 6), ('romance', 6), ('PORN', 6), ('fuck', 6), ('Domestic', 6), ('Ep', 5), ('Top', 5), ('ft', 5), ('For', 5), ('Is', 5), ('India', 5), ('Ka', 5), ('love', 5), ('story', 5), ('NEW', 5), ('Episode', 5), ('Music', 5), ('violence', 5), ('protests', 5), ('George', 5), ('Documentary', 5), ('GUN', 5), ('shot', 5), ('Side', 5), ('crash', 5), ('Brutalities', 5), ('rape', 5), ('Annoying', 5), ('Orange', 5), ('Friend', 4), ('Prank', 4), ('Movies', 4), ('You', 4), ('Special', 4), ('Girl', 4), ('Rose', 4), ('OF', 4), ('trafficking', 4), ('SEX', 4), ('Her', 4), ('When', 4), ('Scene', 4), ('Shocking', 4), ('Story', 4), ('Bollywood', 4), ('Status', 4), ('boy', 4), ('Case', 4), ('NERF', 4), ('Near', 4), ('North', 4), ('Man', 4), ('Dial', 4), ('Adult', 3), ('Kavita', 3), ('Not', 3), ('ENG', 3), ('SUB', 3), ('First', 3), ('Me', 3), ('Phone', 3), ('It', 3), ('TikTok', 3), ('Hollywood', 3), ('English', 3), ('Your', 3), ('During', 3), ('House', 3), ('StandUp', 3), ('Yoga', 3), ('Show', 3), ('Photos', 3), ('News', 3), ('Comedy', 3), ('Lesbian', 3), ('SU', 3), ('Watch', 3), ('Rhea', 3), ('Nisha', 3), ('Viral', 3), ('Tik', 3), ('Tok', 3), ('star', 3), ('ðŸŒŸ', 3), ('Namrata', 3), ('parija', 3), ('reality', 3), ('kapoor', 3), ('porn', 3), ('XXX', 3), ('Couple', 3), ('Action', 3), ('comedy', 3), ('raghav', 3), ('What', 3), ('girl', 3), ('OFFICIAL', 3), ('Murder', 3), ('Women', 3), ('BBC', 3), ('City', 3), ('Floyds', 3), ('On', 3), ('Child', 3), ('War', 3), ('SEAL', 3), ('Black', 3), ('Gang', 3), ('Killed', 3), ('Woman', 3), ('Mortal', 3), ('Kombat', 3), ('brutality', 3), ('Album', 3), ('TRAILER', 3), ('Aunty', 2), ('indian', 2), ('Real', 2), ('Gold', 2), ('Digger', 2), ('Hard', 2), ('Kim', 2), ('birthday', 2), ('Okay', 2), ('Teen', 2), ('Challenge', 2), ('KISS', 2)]\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r',errors='ignore')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "    \n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "add_doc_to_vocab('adult.csv', vocab)\n",
    "add_doc_to_vocab('violence.csv', vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(200))\n",
    "\n",
    "min_occurance = 3\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert entire data into data ready to feed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r',errors='ignore')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    rtr = []\n",
    "    split_text = doc.split(sep='\\n')\n",
    "    for tokens in split_text:\n",
    "        tokens = tokens.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        if len(tokens) > 0:\n",
    "            rtr.append(tokens)\n",
    "    return rtr\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    #data = '\\n'.join(lines)\n",
    "    file = open(filename, 'a')\n",
    "    for line in lines:\n",
    "        if len(line) >=1 :\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    final = []\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    print(len(tokens))\n",
    "    # filter by vocab\n",
    "    for i in range (len(tokens)):\n",
    "        t = [w for w in tokens[i] if w in vocab]\n",
    "        final.append(' '.join(t))\n",
    "    return final\n",
    "         \n",
    "# load all docs in a directory\n",
    "def process_docs(file_name, vocab):\n",
    "    \n",
    "    line = doc_to_line(file_name, vocab)\n",
    "    # add to list\n",
    "    return line\n",
    " \n",
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# prepare negative reviews\n",
    "adult_lines = process_docs('adult.csv', vocab)\n",
    "save_list(adult_lines, 'adult.txt')\n",
    "# prepare positive reviews\n",
    "violence_lines = process_docs('violence.csv', vocab)\n",
    "save_list(violence_lines, 'violence.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
